<!DOCTYPE html>
<html>
<head>
    <title>Page Rank</title>
</head>
<body>
    <h1>Google Page Rank Algorithm</h1>
    <h3>Lauren Lee and Gitae Park</h3>
    <a href="https://docs.google.com/presentation/d/1H05RqEUTWJk2_aDIjigYyyw7RKO1OPdjkyXMxclNN34/edit?usp=sharing">Presentation</a>
    <h3> History </h3>
    <p>
        The Google Page Rank Algorithm revolutionized the search engine industry.
    </p>
    <h4>Early Text Based Algorithms</h4>
    <p>
        In the early 90’s, the first search engines used a text based ranking system which would return pages with the highest number of occurrences of a certain keyword that was inputted. However this not only led to the return of many low-quality matches (for example pages with just the keyword being repeated over and over again) but also allowed advertisers to exploit the algorithm by populating web pages with “invisible keywords” which are text that can’t be seen by regular visitors to the page but can be seen by search engines. 
    </p>
    <h4>Here Comes the Page Rank Algorithm</h4>
    <p>
        So comes Larry Page and Sergey Brin who decide to create an algorithm called PageRank that would objectively measure the relative importance of web pages by computing a ranking for every web page.   Google decides to determine the importance of a web page by using the network structure of the Web by taking into account two things: the number of web pages that links to the certain web page and the PageRank score or the importance of the web pages themselves that refer to the certain web page. We can understand how this works by looking at the Google Page Rank Algorithm.
    </p>
    <h3> The Algorithm </h3>
    <h4>Random Surfer Model</h4>
    <p>
        The <strong>Random Surfer model</strong> is based on the idea of a random surfer on the web who when randomly given a webpage continually randomly clicks on links in that web page to get to new pages. If we let a random surfer keep doing that and increase the score of each webpage the surfer visits, eventually the page rank scores will stabilize. Probability wise, a page that has more pages linking to that page will have a high page rank and a page has a highly authoritative page linking to that page will have a high page rank
    </p>
    <img src="static/surfer.jpg" alt="surfer">
    <h4>Math!!</h4>
    <img src="static/algo.jpg" alt="algo">
    <p>Top half of the image is w/out damping factor. Second half is with damping factor</p>
    <p>
        The page rank of a site is the summation of the fraction of the page ranks of every inbound link. Since page rank is essentially the probability that a random surfer will land on your page, it is calculated by a summation of every possible way a user can reach your page. The reason you must divide the page rank of the inbound link by the number of links on the page with the inbound link is because you are essentially attempting to find the probability that you will first be on a page with an inbound link (page rank) and then clicking on that inbound link (dividing the page rank by the number of outgoing links on that page). Since the page ranks of a page depend on the page ranks of other pages, the page rank of a page must be calculated multiiple times recursively using new and updated values of page ranks. Eventually the page rank will stabilize. Note that when calculating the page rank of a page, the page rank of all pages begin with 1/N, N being the total number of pages in the web of pages. If a page has no outgoing links, you must contribute 1/(N-1) * the pages page rank to every page in the network.
    </p>
    <h6>Damping Factor</h6>
    <p>
        But if we stop there, we fail to consider the problem that pages in the network might not all be connected to each other, getting the random surfer to get stuck in a certain loop. In order to solve this, we introduce something called the damping factor which is the probability that a random surfer will follow a link on the page it is on. For example if the damping factor is 0.85, 85% of the times the surfer will follow a link while 15% of the time the surfer will jump to a random page in the network. This ensures that all web pages will eventually be visited by the random surfer.
    </p>
    <h3>Results</h3>
    <img src="static/results.png" alt="results">
    <p>
        This graph shows us a stabilized result of recursive calculation of PageRank for this network. As you can see, web page B is a more important webpage, having more links pointing to it. And even though web page E has more links pointing to it than web page C, web page C has a higher score because web page B, the most important web page, is linking to it, showing us the effectiveness of the PageRank algorithm.
    </p>
    <h3>Web Crawling</h3>
    Google has a database of webpages that can be accessed when searching using their search engine. However, Google isn't notified of new webpages so they have send out webcrawlers to update their databases. Crawlers will index and render the page to identify metrics like page ranks and hitlists. Hit lists include code words determined by position, capitalization, and font to produce relevant results to keyword searchs. 
</body>
</html>
